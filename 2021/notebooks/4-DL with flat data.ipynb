{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "4-DL with flat data.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F08wdUUQXED7"
      },
      "source": [
        "## Predicting flat (tabular) data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bVU1HRgrJULQ"
      },
      "source": [
        "Each pattern is composed by a fixed number of features (converted into numbers!).  \n",
        "That's why **tabular**: each pattern can be seen as a 2D table (flat)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8znk-GAhJoS7"
      },
      "source": [
        "## Exercise: Boston Housing price regression dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2y3vFLX2KARD"
      },
      "source": [
        "Very popular toy dataset: https://keras.io/api/datasets/boston_housing/  \n",
        "Already provided by Keras for you."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7NKaVT0DLh4A"
      },
      "source": [
        "Try to tackle this problem with a Deep Learning model built with Keras. \n",
        "\n",
        "**Objectives**: \n",
        "*   make sure you are able to build a deep learning model \n",
        "*   make sure you can code a full training loop with a bit of hyperparameter search (model selection vs. model assessment)\n",
        "*   make sure you are able to monitor training progress\n",
        "*   make sure you can apply a minimum level of preprocessing (try to explore the dataset and see what kind of features you are dealing with)\n",
        "*   make sure you can evaluate your model on unseen data\n",
        "*   make sure you are able to save your model and load it back\n",
        "\n",
        "This will be needed when moving forward on more complex tasks on structured data like images, sequences etc. You can reuse the code so that you can then focus on the specific task instead of the *boilerplate* code part which is always the same.\n",
        "\n",
        "**Optional**: feel free to explore other flat datasets!  \n",
        "TF datasets: https://www.tensorflow.org/datasets/catalog/overview#all_datasets  \n",
        "UCI datasets: https://archive.ics.uci.edu/ml/index.php\n",
        "\n",
        "For example (from UCI): \n",
        "\n",
        "HIGGS (~2.6 GB) https://archive.ics.uci.edu/ml/datasets/HIGGS  \n",
        "Binary classification task to distinguish between a signal process which produces Higgs bosons and a background process which does not.\n",
        "**Large dataset**: 11 000 000 patterns, 28 real-valued features\n",
        "\n",
        "Bank Marketing Dataset (use `bank_full.csv` file for patterns) https://archive.ics.uci.edu/ml/datasets/Bank+Marketing  \n",
        "Binary classification task to predict if a client will subscribe a term deposit.  \n",
        "It has 45211 patterns with 17 features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8SNni3PfKMFq"
      },
      "source": [
        "import tensorflow.keras as K\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras.datasets import boston_housing"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uKyxA4bkKVDX"
      },
      "source": [
        "# 2 tuples, each of which containing 2 numpy arrays\n",
        "(x, y), (x_test, y_test) = boston_housing.load_data(test_split=0.15)\n",
        "print(x.shape, y.shape, x_test.shape, y_test.shape)\n",
        "print(x.dtype, y.dtype, x_test.dtype, y_test.dtype)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x5Lt5l9mKoeV"
      },
      "source": [
        "Quite a small dataset, but it's enough for us to get started.\n",
        "\n",
        "Do your best!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iR3aTXnvLfCr"
      },
      "source": [
        "## Solution example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_8o4Ibs8bYMt"
      },
      "source": [
        "Split your dataset!!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_F9Te3VbXCK"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "VAL_SPLIT = 0.15\n",
        "\n",
        "# why don't we use stratify here?\n",
        "x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=VAL_SPLIT, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yUmyNOIEcXGR"
      },
      "source": [
        "Normalize your data!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F4FbfKlSd0NA"
      },
      "source": [
        "print(np.mean(x_train), np.var(x_train))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aqAoLOfOcheI"
      },
      "source": [
        "from tensorflow.keras.layers.experimental.preprocessing import Normalization\n",
        "norm_layer = Normalization(axis=-1)\n",
        "norm_layer.adapt(x_train)\n",
        "normalized_x_train, normalized_x_val, normalized_x_test = norm_layer(x_train), norm_layer(x_val), norm_layer(x_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pjnnmct6dorx"
      },
      "source": [
        "print(np.mean(normalized_x_train), np.var(normalized_x_train))\n",
        "print(np.mean(normalized_x_val), np.var(normalized_x_val)) # of course, this is a less precise normalization"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cwlb6dDVcvjF"
      },
      "source": [
        "Build the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BNMdq6ufaYYf"
      },
      "source": [
        "def get_compiled_model(hidden_size, learning_rate):\n",
        "  optimizer = K.optimizers.SGD(learning_rate=learning_rate, momentum=0.9)\n",
        "  model = K.Sequential()\n",
        "  model.add(K.layers.Input(shape=(13,)))\n",
        "  model.add(K.layers.Dense(hidden_size, activation='tanh'))\n",
        "  model.add(K.layers.Dense(1))\n",
        "  model.compile(optimizer=optimizer, loss='mse')\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JQ53h1QnnQ1-"
      },
      "source": [
        "model = get_compiled_model(256, 1e-4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJ3eEjsrcRkp"
      },
      "source": [
        "model.fit(normalized_x_train, y_train, epochs=10, batch_size=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qWoXrVhfeUjV"
      },
      "source": [
        "metrics = model.evaluate(normalized_x_val, y_val)\n",
        "print(metrics)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OnuwIIeakgc1"
      },
      "source": [
        "## Grid search for model selection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aCw3oF8sleji"
      },
      "source": [
        "from sklearn.model_selection import ParameterGrid\n",
        "\n",
        "# 6 configurations total\n",
        "choices = {\"learning_rate\": [1e-2, 1e-3, 1e-4], \"hidden_size\": [128, 256]}\n",
        "grid = ParameterGrid(choices)\n",
        "print(len(list(grid)), list(grid))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I7CIlcLAoBj-"
      },
      "source": [
        "best_loss = 100000\n",
        "best_conf = None\n",
        "for el in grid:\n",
        "  print(\"Training with configuration: \", el[\"learning_rate\"], el[\"hidden_size\"])\n",
        "  model = get_compiled_model(el[\"hidden_size\"], el[\"learning_rate\"])\n",
        "  model.fit(normalized_x_train, y_train, epochs=10, batch_size=10, verbose=0)\n",
        "  loss = model.evaluate(normalized_x_val, y_val)\n",
        "  print(\"Loss: \", loss)\n",
        "  if loss < best_loss:\n",
        "    print(\"Found better configuration\")\n",
        "    best_loss = loss\n",
        "    best_conf = el\n",
        "print(best_loss)\n",
        "print(best_conf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8vaDViFn3I9"
      },
      "source": [
        "## Model assessment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3qNgUF0uqVh2"
      },
      "source": [
        "You can retrain on the union of training and validation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CuV4Z735oFR2"
      },
      "source": [
        "normalized_x = norm_layer(x)\n",
        "model = get_compiled_model(el[\"hidden_size\"], best_conf[\"learning_rate\"])\n",
        "model.fit(normalized_x, y, epochs=10, batch_size=10)\n",
        "loss = model.evaluate(normalized_x_test, y_test)\n",
        "print(\"Final loss: \", loss)\n",
        "\n",
        "model.save('model.h5')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}