{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "7-Generative + Graphs.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kcHKwvgZHZmh"
      },
      "source": [
        "# Generative Deep Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SfNWtpe_nvI8"
      },
      "source": [
        "Up to now, we mostly focused on classification and regression tasks in the context of deep learning. \n",
        "\n",
        "However, an important avenue of research is the *generation* of patterns with deep learning models.\n",
        "\n",
        "Two of the most famous generative models are the Generative Adversarial Network (GAN) and the Variational Autoencoder (VAE).  \n",
        "We will look at [an example](https://keras.io/examples/generative/dcgan_overriding_train_step/) of how to generate faces with a GAN.\n",
        "\n",
        "**Warning**: these beasts are hard to train!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6HyXSgF9oVM-"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.keras as K\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import gdown\n",
        "from zipfile import ZipFile"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S4lRUB-GokEE"
      },
      "source": [
        "Generative models need a dataset from which learn what to generate!  \n",
        "[CelebA dataset](http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html) provides a large number of examples of... faces!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YKg0hYEhohfn"
      },
      "source": [
        "os.makedirs(\"celeba_gan\")\n",
        "\n",
        "url = \"https://drive.google.com/uc?id=1O7m1010EJjLE5QxLZiM9Fpjs7Oj6e684\"\n",
        "output = \"celeba_gan/data.zip\"\n",
        "gdown.download(url, output, quiet=True)\n",
        "\n",
        "with ZipFile(\"celeba_gan/data.zip\", \"r\") as zipobj:\n",
        "    zipobj.extractall(\"celeba_gan\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KR0z0PDkpGB8"
      },
      "source": [
        "Let's look at what we've got. We can exploit Keras to easily build a dataset ouf of it.\n",
        "\n",
        "Remember also to normalize data!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PFdItf3co-H9"
      },
      "source": [
        "dataset = K.preprocessing.image_dataset_from_directory(\n",
        "    \"celeba_gan\", label_mode=None, image_size=(64, 64), batch_size=32\n",
        ")\n",
        "dataset = dataset.map(lambda x: x / 255.0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FsJfg2T5pV9_"
      },
      "source": [
        "We don't need labels... Why?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0IKEla1apXgE"
      },
      "source": [
        "for x in dataset: # no y!\n",
        "  print(x.shape)\n",
        "  print(np.max(x), np.min(x))\n",
        "  break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SYtbSL2mp2Ek"
      },
      "source": [
        "plt.imshow(x[0])\n",
        "plt.axis('off')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3dr6y6y9qCe2"
      },
      "source": [
        "## Let's build the model!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5jf4eQR4qOsT"
      },
      "source": [
        "**DISCRIMINATOR**: the one that has to decide if an image is real or fake\n",
        "\n",
        "Binary classification from images! Easy!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6N-cJHpKqkyT"
      },
      "source": [
        "discriminator = K.Sequential(name='discriminator')\n",
        "discriminator.add(K.Input(shape=(64, 64, 3)))\n",
        "discriminator.add(K.layers.Conv2D(64, kernel_size=4, strides=2, padding=\"same\"))\n",
        "discriminator.add(K.layers.LeakyReLU(alpha=0.2))\n",
        "discriminator.add(K.layers.Conv2D(128, kernel_size=4, strides=2, padding=\"same\"))\n",
        "discriminator.add(K.layers.LeakyReLU(alpha=0.2))\n",
        "discriminator.add(K.layers.Conv2D(128, kernel_size=4, strides=2, padding=\"same\"))\n",
        "discriminator.add(K.layers.LeakyReLU(alpha=0.2))\n",
        "     \n",
        "discriminator.add(K.layers.Flatten())\n",
        "discriminator.add(K.layers.Dropout(0.2))\n",
        "discriminator.add(K.layers.Dense(1, activation=\"sigmoid\")) # sigmoid is enough\n",
        "\n",
        "discriminator.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qUw8Z500rFKO"
      },
      "source": [
        "**GENERATOR**: generate a face from random noise"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "os-B3zB2rTsC"
      },
      "source": [
        "latent_dim = 128 # size of the discriminator before flattening\n",
        "\n",
        "generator = K.Sequential(name='generator')\n",
        "generator.add(K.Input(shape=(latent_dim,)))\n",
        "generator.add(K.layers.Dense(8 * 8 * 128))\n",
        "generator.add(K.layers.Reshape((8, 8, 128))) # set this to match the discriminator final feature map\n",
        "generator.add(K.layers.Conv2DTranspose(128, kernel_size=4, strides=2, padding=\"same\"))\n",
        "generator.add(K.layers.LeakyReLU(alpha=0.2))\n",
        "generator.add(K.layers.Conv2DTranspose(256, kernel_size=4, strides=2, padding=\"same\"))\n",
        "generator.add(K.layers.LeakyReLU(alpha=0.2))\n",
        "generator.add(K.layers.Conv2DTranspose(512, kernel_size=4, strides=2, padding=\"same\"))\n",
        "generator.add(K.layers.LeakyReLU(alpha=0.2))\n",
        "generator.add(K.layers.Conv2D(3, kernel_size=5, padding=\"same\", activation=\"sigmoid\"))\n",
        "\n",
        "generator.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t7U3-ri-sEuZ"
      },
      "source": [
        "**The trickier part**: how to train the entire GAN?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jrGOBFsasLx4"
      },
      "source": [
        "We have to build a custom Keras Model which encapsulates both the generator and the discriminator and train them accordingly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46-r8bskrxtZ"
      },
      "source": [
        "class GAN(K.Model):\n",
        "    def __init__(self, discriminator, generator, latent_dim):\n",
        "        super(GAN, self).__init__()\n",
        "        self.discriminator = discriminator\n",
        "        self.generator = generator\n",
        "        self.latent_dim = latent_dim\n",
        "\n",
        "    def compile(self, d_optimizer, g_optimizer, loss_fn):\n",
        "        super(GAN, self).compile()\n",
        "        self.d_optimizer = d_optimizer\n",
        "        self.g_optimizer = g_optimizer\n",
        "        self.loss_fn = loss_fn\n",
        "        \n",
        "        # use standalone metrics to maintain a mean loss\n",
        "        # for both the models\n",
        "        self.d_loss_metric = K.metrics.Mean(name=\"d_loss\")\n",
        "        self.g_loss_metric = K.metrics.Mean(name=\"g_loss\")\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [self.d_loss_metric, self.g_loss_metric]\n",
        "\n",
        "    def train_step(self, real_images):\n",
        "        # GENERATE RANDOM INITIAL VECTOR FOR THE GENERATOR\n",
        "        batch_size = tf.shape(real_images)[0]\n",
        "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
        "\n",
        "        # GENERATE FAKE IMAGES\n",
        "        generated_images = self.generator(random_latent_vectors)\n",
        "\n",
        "        # MIX THE DATA\n",
        "        combined_images = tf.concat([generated_images, real_images], axis=0)\n",
        "        labels = tf.concat(\n",
        "            [tf.ones((batch_size, 1)), tf.zeros((batch_size, 1))], axis=0\n",
        "        ) # 0=real, 1=fake\n",
        "        # Label smoothing: ease convergence\n",
        "        labels += 0.05 * tf.random.uniform(tf.shape(labels))\n",
        "\n",
        "        # Train the discriminator\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = self.discriminator(combined_images)\n",
        "            d_loss = self.loss_fn(labels, predictions)\n",
        "        grads = tape.gradient(d_loss, self.discriminator.trainable_weights)\n",
        "        self.d_optimizer.apply_gradients(\n",
        "            zip(grads, self.discriminator.trainable_weights)\n",
        "        )\n",
        "\n",
        "        # Sample random points in the latent space\n",
        "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
        "\n",
        "        # Assemble labels that say \"all real images\"\n",
        "        misleading_labels = tf.zeros((batch_size, 1))\n",
        "\n",
        "        # Train the generator (note that we should *not* update the weights\n",
        "        # of the discriminator)!\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = self.discriminator(self.generator(random_latent_vectors))\n",
        "            g_loss = self.loss_fn(misleading_labels, predictions)\n",
        "        grads = tape.gradient(g_loss, self.generator.trainable_weights)\n",
        "        self.g_optimizer.apply_gradients(zip(grads, self.generator.trainable_weights))\n",
        "\n",
        "        # Update metrics\n",
        "        self.d_loss_metric.update_state(d_loss)\n",
        "        self.g_loss_metric.update_state(g_loss)\n",
        "        return {\n",
        "            \"d_loss\": self.d_loss_metric.result(),\n",
        "            \"g_loss\": self.g_loss_metric.result(),\n",
        "        }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "izQOd_hatVo5"
      },
      "source": [
        "If you want to know more about the label trick, you can look [here](https://www.inference.vc/instance-noise-a-trick-for-stabilising-gan-training/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XX2DWR2yt3oy"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_LVxH6yx67Y"
      },
      "source": [
        "**Warning**: this will take quite a lot of time on the entire dataset. You can consider to reduce the dataset size with `take`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3jx6mkGEt-gI"
      },
      "source": [
        "epochs = 2\n",
        "MAX_DATASET_SIZE = 10000 # -1 to take entire dataset\n",
        "\n",
        "gan = GAN(discriminator=discriminator, generator=generator, latent_dim=latent_dim)\n",
        "gan.compile(d_optimizer=K.optimizers.Adam(learning_rate=0.0001), \n",
        "            g_optimizer=K.optimizers.Adam(learning_rate=0.0001), \n",
        "            loss_fn=K.losses.BinaryCrossentropy())\n",
        "\n",
        "dataset = dataset.take(MAX_DATASET_SIZE)\n",
        "gan.fit(dataset, epochs=epochs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1du2rV2luQ2X"
      },
      "source": [
        "## Results!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-qOF_wIzuSRm"
      },
      "source": [
        "def generate_faces(generator, latent_dim, num_faces=6):\n",
        "  random_latent_vectors = tf.random.normal(shape=(num_faces, latent_dim))\n",
        "  generated_images = generator(random_latent_vectors) # these are in [0, 1]\n",
        "  generated_images.numpy()\n",
        "  return generated_images "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bycXkW5hufyg"
      },
      "source": [
        "num_faces = 6\n",
        "faces = generate_faces(generator, latent_dim, num_faces)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XaBOtg4PutSR"
      },
      "source": [
        "fig, ax = plt.subplots(1, num_faces)\n",
        "for i, face in enumerate(faces):\n",
        "  ax[i].imshow(face)\n",
        "  ax[i].axis('off')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HbUYw8MRzK6E"
      },
      "source": [
        "**Mode collapse**: produce similar outputs! There is not enough diversity in the restricted dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yG0mO1g5zPVs"
      },
      "source": [
        "**Exercise**: build a character-level RNN generator. Generate sentences character by character from a fixed-size alphabet.\n",
        "\n",
        "E.g. take the english alphabet plus some punctuation symbol (+ start and end of sequence). Train your RNN on a corpus of text, character by character. Train it to predict the next character until the END is encountered. Then, try to generate text starting from START until END is produced."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0Dvoyl4zZnG"
      },
      "source": [
        "# Deep Graph Networks (aka Graph Neural Networks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lqE7lc0vzdAv"
      },
      "source": [
        "[Spektral](https://graphneural.network/) is a high-level library based on Keras for graph applications.\n",
        "\n",
        "We don't have time to dive into many details. Here, I just wanted to show you how easily it can be to build a complex model leveraging Keras and Spektral functionalities."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H5b1VE5z1FUB"
      },
      "source": [
        "[The task](https://arxiv.org/pdf/1609.02907.pdf) we will consider is a node classification task. Each node in the graph has to be associated to a specific class."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VAic8XCf1dsh"
      },
      "source": [
        "Let's first install Spektral!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MUjZnUhm1ftl"
      },
      "source": [
        "!pip install spektral"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1pqVbjuy1oR3"
      },
      "source": [
        "from spektral.data.loaders import SingleLoader\n",
        "from spektral.datasets.citation import Citation\n",
        "from spektral.layers import GCNConv\n",
        "from spektral.models.gcn import GCN\n",
        "from spektral.transforms import AdjToSpTensor, LayerPreprocess"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZuK5FW5w1yIO"
      },
      "source": [
        "We will work with Citation networks, specifically the [Cora dataset](https://graphsandnetworks.com/the-cora-dataset/), check it out!  \n",
        "\"*The Cora dataset consists of 2708 scientific publications classified into one of seven classes. The citation network consists of 5429 links. Each publication in the dataset is described by a 0/1-valued word vector indicating the absence/presence of the corresponding word from the dictionary. The dictionary consists of 1433 unique words.*\"\n",
        "\n",
        "So, nodes are documents and links are citations.  \n",
        "Node classification task requires to predict the label of a paper among 7 possible classes (paper subject area)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rQCDmcxm2bqV"
      },
      "source": [
        "dataset = Citation(\"cora\", normalize_x=True, \n",
        "                   transforms=[LayerPreprocess(GCNConv),  # the model we will use needs to preprocess the adj matrix\n",
        "                                                          # GNCConv implements a preprocess method which is called in the transform\n",
        "                               AdjToSpTensor()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9UhaGYHS4LS7"
      },
      "source": [
        "dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FeggXHcI5YHC"
      },
      "source": [
        "this is not the usual dataset you are accustomed to. This is what is called `Single Data Mode` in Spektral (only one graph, usually for node classification tasks)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HaALPPy_5bTk"
      },
      "source": [
        "dataset[0] # take the graph"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZtCZgPhw5dVf"
      },
      "source": [
        "we have some nodes and some links with associated features and labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JMUsMIwx5hAG"
      },
      "source": [
        "print(dataset[0].a.shape) # adj matrix\n",
        "print(dataset[0].x.shape) # feature nodes (1433 feature per node indicating presence/absence of word)\n",
        "# print(dataset[0].e.shape) # edge features, we don't have them\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K8FkJOii7bQM"
      },
      "source": [
        "print(dataset[0].a)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nnq-9IsH8J9f"
      },
      "source": [
        "**Create the Graph Convolutional Network model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gBpHxH7f8F4U"
      },
      "source": [
        "model = GCN(n_labels=dataset.n_labels, n_input_channels=dataset.n_node_features)\n",
        "\n",
        "model.compile(optimizer=K.optimizers.Adam(learning_rate=1e-2), \n",
        "              loss=K.losses.CategoricalCrossentropy(reduction=\"sum\"), # take the sum, not the mean across nodes\n",
        "              metrics=[\"accuracy\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IcTw-O6G8zwM"
      },
      "source": [
        "Split dataset into training, validation and test using Spektral"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zb6GbpdB9O4f"
      },
      "source": [
        "loader_tr = SingleLoader(dataset, sample_weights=dataset.mask_tr)\n",
        "loader_va = SingleLoader(dataset, sample_weights=dataset.mask_va)\n",
        "loader_te = SingleLoader(dataset, sample_weights=dataset.mask_te)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWBs2cFd9WpH"
      },
      "source": [
        "## Training the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7e40AYqQ0G_S"
      },
      "source": [
        "model.fit(loader_tr.load(),\n",
        "          steps_per_epoch=loader_tr.steps_per_epoch,\n",
        "          validation_data=loader_va.load(),\n",
        "          validation_steps=loader_va.steps_per_epoch,\n",
        "          epochs=200,\n",
        "          callbacks=[K.callbacks.EarlyStopping(patience=10, restore_best_weights=True)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nM5bLcd-9pNH"
      },
      "source": [
        "eval_results = model.evaluate(loader_te.load(), steps=loader_te.steps_per_epoch)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}