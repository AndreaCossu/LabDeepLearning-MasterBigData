{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3-Advanced.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R71uaXuN_FTC"
      },
      "source": [
        "# ADVANCED TOPICS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pO8oDqF0_MWc"
      },
      "source": [
        "* Build a training loop with a custom training step\n",
        "  * Useful to build custom models!\n",
        "* Explore DL tricks and how to implement them with Keras\n",
        "  * Useful to improve performances"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25RZsQhh_bwL"
      },
      "source": [
        "## CUSTOM TRAINING STEP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nLTtL6U-7nXR"
      },
      "source": [
        "from tensorflow import keras as K\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Uc8xQ8h75DE"
      },
      "source": [
        "### LET'S BUILD OUR USUAL DATASET"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TTDDkVv77zaR"
      },
      "source": [
        "N_CLASSES = 3\n",
        "N_PATTERNS_PER_CLASS = 5000\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "N_PATTERNS = N_CLASSES * N_PATTERNS_PER_CLASS\n",
        "X, y = make_classification(n_samples=N_PATTERNS, n_classes=N_CLASSES, n_informative=5)\n",
        "test_size = int(0.25 * y.shape[0])\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, shuffle=True, stratify=y)\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(buffer_size=1024).batch(BATCH_SIZE)\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(BATCH_SIZE)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MYSzjxBB8OL5"
      },
      "source": [
        "### LET'S BUILD OUR USUAL MODEL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fVsRn-EC8Tuj"
      },
      "source": [
        "inputs = K.Input(shape=(20,)) # input layer\n",
        "h = K.layers.Dense(units=64, activation='relu')(inputs) # hidden layer\n",
        "outputs = K.layers.Dense(units=N_CLASSES, activation='softmax')(h) # output layer\n",
        "model = K.Model(inputs=inputs, outputs=outputs)\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "THuBaxht8bfZ"
      },
      "source": [
        "### KERAS TRAINING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "93i4EB7b8eLG"
      },
      "source": [
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
        "print(\"Training\")\n",
        "model.fit(train_dataset, epochs=10) # batch size is already specified by the dataset\n",
        "print(\"Evaluating\")\n",
        "metrics = model.evaluate(test_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZOWAflYP_nOM"
      },
      "source": [
        "Up to now all good. Let's dig deeper!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "APlOprgg_rr0"
      },
      "source": [
        "### IMPLEMENT CUSTOM TRAINING METHOD"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pD09nfCB_vsO"
      },
      "source": [
        "In order to do that, we have to build a custom model!  \n",
        "We also have to specify which metrics we want to monitor.\n",
        "\n",
        "The `train_step` method trains your model on a single minibatch of data. Do not override `fit`, since you will need to manage more complex Keras functionalities related to epochs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ip0K-EqkrjdW"
      },
      "source": [
        "# this has to be a loss, otherwise gradients won't be tracked\n",
        "loss_metric = K.losses.SparseCategoricalCrossentropy()\n",
        "\n",
        "acc_metric = K.metrics.Accuracy(name=\"acc\")\n",
        "mean_loss = K.metrics.Mean(name=\"loss\") "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nAbThUZq_k3v"
      },
      "source": [
        "class MyModel(K.Model):\n",
        "  \"\"\"\n",
        "  Model overriding `train_step` method\n",
        "  \"\"\"\n",
        "\n",
        "  def train_step(self, data):\n",
        "    \"\"\"\n",
        "    :param data: depending on what you pass to `fit` method this can be\n",
        "      yielded by a tensorflow dataset or sampled from (X, y) tuple\n",
        "    \"\"\"\n",
        "\n",
        "    # loop over dataset\n",
        "    # this is 1 iteration\n",
        "    x, y = data\n",
        "    with tf.GradientTape() as tape:\n",
        "      # this calls the forward pass\n",
        "      y_pred = self(x, training=True) \n",
        "      loss_value = loss_metric(y, y_pred)\n",
        "      \n",
        "    # # backward pass (backpropagation)\n",
        "    gradients = tape.gradient(loss_value, self.trainable_variables) \n",
        "\n",
        "    # gradient step update\n",
        "    self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
        "\n",
        "    # get classification \n",
        "    winner = tf.argmax(y_pred, axis=-1)\n",
        "    acc_metric.update_state(y, winner)\n",
        "    mean_loss.update_state(loss_value)    \n",
        "\n",
        "    return {\"loss\": mean_loss.result(), \"acc\": acc_metric.result()}\n",
        "\n",
        "\n",
        "  def test_step(self, data):\n",
        "    \"\"\"\n",
        "    Custom evaluation step\n",
        "    \"\"\"\n",
        "    x, y = data\n",
        "    y_pred = self(x, training=False)\n",
        "    loss_value = loss_metric(y, y_pred)\n",
        "    mean_loss.update_state(loss_value)\n",
        "    winner = tf.argmax(y_pred, axis=-1)\n",
        "    acc_metric.update_state(y, winner)\n",
        "    print(\"Inner print: \", x.shape)\n",
        "    return {\"loss\": mean_loss.result(), \"acc\": acc_metric.result()}\n",
        "\n",
        "  @property\n",
        "  def metrics(self):\n",
        "    \"\"\"\n",
        "    Add metrics here. In this way, metrics are automatically\n",
        "    reset at the end of each epoch.\n",
        "    \"\"\"\n",
        "    return [mean_loss, acc_metric]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pe0uxN4wtGG3"
      },
      "source": [
        "inputs = K.Input(shape=(20,)) # input layer\n",
        "h = K.layers.Dense(units=32, activation='relu')(inputs) # hidden layer\n",
        "outputs = K.layers.Dense(units=N_CLASSES, activation='softmax')(h) # output layer\n",
        "model = MyModel(inputs=inputs, outputs=outputs)\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xTx_CZHitRRf"
      },
      "source": [
        "model.compile(optimizer=\"adam\") # metrics are already specified!"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "leeMfvMUtmQN"
      },
      "source": [
        "print(\"Training\")\n",
        "model.fit(train_dataset, epochs=10)\n",
        "print(\"Evaluating\")\n",
        "metrics = model.evaluate(test_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TthSY-wDVmbJ"
      },
      "source": [
        "**Exercise**: Implement an entire training loop (i.e. multiple epochs) outside the model.  \n",
        "Use any model you want and monitor any metrics you like."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JUSXO3yM_kJz"
      },
      "source": [
        "## DL TRICKS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRI3N2Qw_lVx"
      },
      "source": [
        "### L2/L1 REGULARIZATION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LShZ7kwSWEc_"
      },
      "source": [
        "This is useful to reduce the weights values. Large values are usually associated to overfitting. By adding a penalization term to the loss function it is possible to induce weights to have small values.  \n",
        "Depending on the penalization type you can have L1 (absolute value) or L2 (square norm) penalization.  \n",
        "Same process applies to output values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q729HUkjWY1g"
      },
      "source": [
        "l2_reg = K.regularizers.l2(l2=1e-2)\n",
        "l1_reg = K.regularizers.l1(l1=1e-2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S2rF6SE4XtIT"
      },
      "source": [
        "layer = K.layers.Dense(32, kernel_regularizer=l2_reg, activity_regularizer=l1_reg)\n",
        "out = layer(tf.random.normal(shape=(10,7)))\n",
        "layer.losses"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-DS42dz5YW2r"
      },
      "source": [
        "You can easily create your custom regularizer (it is just a function/class)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8oEhfigM_sQR"
      },
      "source": [
        "### EARLY STOPPING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V5s7vbncYbkZ"
      },
      "source": [
        "callback = K.callbacks.EarlyStopping(monitor='acc', patience=3, \n",
        "                                     restore_best_weights=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BPFM2OaiYz5z"
      },
      "source": [
        "Simply use this as a callback during `fit`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ahHj_UOO_w0o"
      },
      "source": [
        "### DROPOUT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQP137mZZM69"
      },
      "source": [
        "Switch off random units during training.  \n",
        "Use all units during evaluation.  \n",
        "This makes the network resilient to different topologies and more robust against overfitting.\n",
        "\n",
        "Place it after activation (but it is not a must)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K2QK8XcoZVzM"
      },
      "source": [
        "model = K.Sequential()\n",
        "# use tanh to clearly separate 0 produced by relu from the ones\n",
        "# produced by the activation function (relu may cause confusion)\n",
        "model.add(K.layers.Input(shape=(2,)))\n",
        "model.add(K.layers.Dense(20, activation=\"tanh\"))\n",
        "model.add(K.layers.Dropout(rate=0.5))\n",
        "input_t = tf.constant([1,2,3,4,5,6,7,8,9,10], shape=(5,2), dtype=tf.float32)\n",
        "print(input_t)\n",
        "print()\n",
        "out = model(input_t, training=True)\n",
        "print(out)\n",
        "print()\n",
        "out2 = model(input_t, training=False)\n",
        "print(out2)\n",
        "\n",
        "# change the rate [0, 1) and see what happens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9HZGVPyY_udD"
      },
      "source": [
        "### BATCH NORMALIZATION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HPzU9ODtdCWX"
      },
      "source": [
        "Keep adaptive mean and std parameters for each layer. Normalize the output of each layer with mean 0 and std 1. These are **learned** parameters!  \n",
        "During training: use the current mean std.  \n",
        "During inference: use the average mean and std.\n",
        "Improves stability of predictions and generalization.\n",
        "Place it between layer and activation.\n",
        "\n",
        "Many hypothesis on why BN improves performance. None has been really proofed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6RVP1dzy_rSj"
      },
      "source": [
        "model = K.Sequential()\n",
        "# use tanh to clearly separate 0 produced by relu from the ones\n",
        "# produced by the activation function (relu may cause confusion)\n",
        "model.add(K.layers.Input(shape=(2,)))\n",
        "model.add(K.layers.Dense(20))\n",
        "model.add(K.layers.BatchNormalization())\n",
        "model.add(K.layers.ReLU())\n",
        "input_t = tf.constant([1,2,3,4,5,6,7,8,9,10], shape=(5,2), dtype=tf.float32)\n",
        "out = model(input_t, training=True)\n",
        "print(out)\n",
        "out2 = model(input_t, training=False)\n",
        "print()\n",
        "print(out2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aeQ6FVCDU97E"
      },
      "source": [
        "**Exercise**: try to empirically validate the effect of these DL tricks on a real dataset (use Keras dataset to get started)."
      ]
    }
  ]
}